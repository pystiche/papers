\section{Difficulty of replicability} \label{sec:replicability}

When replicating the paper from \secref{sec:replicated_paper}, we noticed differences between the approach described in the paper and the original implementation. In most replications, these differences have an impact on the result, either without stylisation at all, the result looks visually different, or only unusable images are created. In general, the deviations can be divided into two categories. The first category is the behavioral changes that probably result from author misconceptions about how his or her used framework or library works internally. The second category hyperparameters result from the different specifications in the documentation and the finally implemented approach. Another influencing factor for this category is the high number of different hyperparameters needed for the implementation. These range from the I/O periphery of the images, to the pre- and postprocessing methods applied, to the specific hyperparameters for the loss functions, which makes it complex to document everything in the paper accurately. Information on both types of changes can be found for each paper implementation in the respective "Behavior Changes" and "Hyperparameters" sections of the documentation. In the following, these categories and subcategories are described in more detail and supported by examples from the replicated papers. The examples do not appear in all replications, but serve only to illustrate the observed deviations in this replication study.

\subsection{Behavioral changes}

The behavioral changes are modified parameters that result from different specifications in the paper and the implementation. However, there should be no influence on these parameters to change the actual style transfer.  

This type of error includes the use of the mean square error (MSE) in the reference implementation, while the square error (SE) is specified in the paper. This category also includes additional norms, such as a division of the number of channels, as well as additional pre-factors. The latter is mainly due to the specification of additional factors in the paper in the loss function which makes the expression of the gradient prettier, but are not integrated by default in the loss functions of the DLF used in the reference implementation. An example of this is a factor $1/2$, which cancels out the $2$ from the square term in the gradient of the MSE loss function. All these examples have an influence on the total loss and thus on the behavior of the algorithm, but are not intended to have a direct influence on it.

Another example of the behavioral changes are changes to the transformer structure in the model-based approaches. This can be different indications of the value range delimiter for the transformer output or different activation functions (LeakyReLU instead of ReLU). There may also be differences in the basic structure of the transformer by using additional or fewer blocks than specified in the paper.

The changes to the transformer structure result from an inaccurate documentation of the model. However, there are also changes that have been deliberately integrated to improve the style transfer without further information in the paper. This was the case with the replacement of maximum pooling operations by average pooling in the pre-trained image processing networks needed for the loss calculation in NST. The idea is to allow the gradient to flow to all input units and thus reduce artefacts during the optimisation process [cite Gatys???].

Since all these behavioral changes are binary, whether from the information in the paper or from the reference implementation, we have implemented both variants and it is possible to switch between them.

\subsection{Hyperparameters}

In contrast to the behavioral changes, the changes of the category Hyperparameter are freely selectable and are suitable for influencing the style transfer positively by adapting. These parameters include the learning rate, the used layers from the pre-trained network, the used loss weights of the individual loss functions, image size, number of optimisation steps, as well as other approach-specific parameters. Unfortunately, there are different information about the parameters used in the paper and the reference implementation. The differences range from different values, different information about the used interpolation or layers, to no specific information about the value at all. However, these parameters are essential for accurate replication. 

One reason for the differences is that it is difficult to properly document all these parameters and know exactly which parameters were used for a particular image/model, as the code is constantly changing before and sometimes after release. It can also become complex to correctly document all standardisations and mean calculations in the formulas in the paper, which may result in additional or missing factors. An example of this is the choice of the weights for the layers. In the formula in the paper a weight of $1/n^{2}$ is given where $n$ denotes the number of channels of a feature map, which is also used in the implementation for the weights. However, without going into detail here, the calculation is already integrated by the use of MSE in the implementation, which means that in the implementation this step is performed twice, but is only specified once in the paper.

Another example for this category is the specification of the used layers from the pre-trained networks that are needed for the loss calculation in NST. There are different statements about the depth of the layers, the number of the layers used and the feature map, i. e. whether the features before or after the activation function are selected. The fact that the choice of layers has an influence on the result has already been noted in \cite{GEB2016}.

Another observed point of this category of changes are different information about important settings, such as the choice of starting point and the number of iteration steps. These are essential parameters for accurate replication. However, it could be that such an important parameter is not specified in the paper and can only be taken from the implementation or two fundamentally different information are given. One example is that instead of a random initialisation with a noise image in the implementation, the content image is chosen as a starting point, which is certainly a practice in the image-based NST. 

All these parameters have an influence on the final result and are necessary to determine if the replication is correct and differences are due to rounding errors or different initialisations. For this reason, similar to the behavioral changes, we have used the information given in the paper and the reference implementation as default values. An overview of the parameters used for the respective replications can be found in the tables in the appendix. In the case of missing information, we have used the available information for both variants. Since these parameters are intended to improve the stylisation, it is also possible that different parameters have been used for a given style image. These variations from the default values are indicated for the respective images or model trainings.

\subsection{Dataset}

There are many datasets with which the models of the model-based approaches can be trained. Some of these datasets exist in several forms, in different resolutions, or they are divided into training-, validation- and testset. However, the replicated papers often do not provide more precise information about the dataset, but only the information about the dataset used. Since the NST is not a training in the typical sense of machine learning, where the result is validated with the validation and testset, the information is missing, since theoretically all sets can be rolled together to increase the dataset size and used for the training of the transformer. There are thus many different combination possibilities, which are increased again if the dataset exists in several resolutions. This is particularly a problem, as we have found that in some of the replicated paper, the much smaller validation set has been used.

Since in most cases the training set has been used for the training, we have done so even if there is no information. An overview of the dataset required for this replication study and the set used in each case can be found in \secref{sec:data}.

\subsection{Transformation of the images}

The images in the datasets or the images used in the image-based methods often do not exist in the size used for the approach. The images are therefore transformed to the size used. Sometimes the transformation is also necessary or useful to make the approach work at all or to provide usable images, such as images with a square image size of $2^{m}$ with $ m \in \mathbb{Z}^{+}$, so that the transformer does not change the image size due to discretisation. 

However, in the paper there is often little or no information about this steps available. But in the implementation, the images are resized to the final image size by several transformations, such as resizing and cropping. This also applies to training time. The training images used are modified during the training by numerous image augmentation. This means that the used dataset is not only resized to the right size, but also artificially enlarged by random transformations, such as rotation, translation and scaling, but also random changes in the light level or brightness of the images. Especially the latter may have an influence on the way the style is presented, as it is available in different sizes and orientations for the training. As a result, it is not possible to trace exactly how the used image was created. In addition, the training in the model-based methods is altered by the artificial enlargement of the dataset, which may also have an influence on the result.

For this replication study, we have re-implemented these transformations and use them for the reference implementation. However, since this is a comprehensible step even without information, we use a minimally necessary process, in the case that no further information is given. This means that if no further information is given, the images are resized to the required image size by resizing with a bilinear interpolation.

\subsection{Pre- and Postprocessing}

Another point are the necessary pre- and postprocessing steps for the pre-trained networks used. This data preparation involves a standardisation with the network-specific mean and standard derivation. However, the papers rarely provide information on whether and when this step will be taken. This makes it difficult to trace this step, especially with model-based approaches, so that different implementations can occur. There are three different ways to integrate preprocessing in model-based approaches. On the one hand, this step can be performed before the transformer, which means that the images have to be pre- and post-processed even during inference. Alternatively, this step can be separated from the transformer and applied only to the images that go into the pre-trained network. This variant has the advantage that pre- and postprocessing during inference is eliminated. A third variant is created by ignoring the preprocessing of the images created by the transformer. This means that the transformer must learn to perform the preprocessing itself. This variant is impractical, since only an inaccurate variant of the preprocessing is learned and postprocessing is required during inference. However, we also found this variant in the replicated paper.

Pre- and postprocessing are steps that are implied by the pre-trained networks used, so we have integrated this step at the places where the original authors have done this step.

\subsection{Custom backward pass}

In the early days of NST, the forward and backward pass had to be implemented, which in part led to current DLF to divergent behavior.

Integrating these changes into the current DLF is difficult, as they require manipulation of the existing backward pass. But we have implemented this in the cases.

%In the early days of NST there were no frameworks with auto-differen\-tia\-ti\-on. Therefore, the available frameworks were used, in which both the forward pass and the backward pass have to be implemented. An example is the currently no longer used framework \texttt{luaTorch}. However, in most decent deep learning libraries (\texttt{PyTorch}, \texttt{Tensorflow}, \texttt{Caffe}, \texttt{Keras}, \texttt{MxNet} etc), it is implemented as the reverse mode of auto-differentiation. This means that the operations are cashed on the forward pass and to have coded the derivatives of simple functions like addition, multiplication, exponentiation etc. This allows even complex functions to be written as a composition of basic functions, allowing the platform to rewrite the functions this way and thus is able to compute gradients in an automatic manner \cite{BPRS2018}. The problem with this development is that in the early days the forward pass and the backward pass were not always implemented as they are with the auto-differentiation in the current DLF. This has no effect for linear operations and only a wrong order, but for nonlinear operations and additional or fewer operations in the backward pass this can have an impact on the result. An example of this is a gradient normalisation in the backward pass.



